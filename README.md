# üí° Samsung Computer Engineering Challenge üí°
- Team name: ÏÑúÍµêÏàòÎÑ§ ÎùºÎßàÎÜçÏû•
- Affiliation: Computer Systems Lab. (CSL), Sungkyunkwan University
- Members: Junyeol Yu, Gwanjong Park, Khan Osama
- E-mail: junyeol.yu@skku.edu, jesj74@g.skku.edu, khan980@g.skku.edu
- Challenge site: [[link]](https://cechallenge.github.io/)
<br>

# LLaMA Baseline

This repository is forked from the [repository](https://github.com/facebookresearch/llama) intended as a minimal, hackable and readable example to load [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) ([arXiv](https://arxiv.org/abs/2302.13971v1)) models and run inference.

It uses tensor parallelism technique to perform inference with the LLaMA-30B model.

The performance of the baseline technique applied to the Hellaswag validation-zeroshot task is measured using the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

> Tensor parallelism refers to dividing the tensors that make up the model into multiple chunks horizontally and allocating each chunk to a single GPU for parallel processing (multi-GPU setting). After each layer calculation, the tensor processing results are generated by synchronizing the output values of different GPUs to prepare the input for the next layer.

## Features
- Suited for Hellaswag task (zero-shot, sequence classification)
- Removed auto-regressive decode (generation) module
- Calculating prediction class from prefill module's output

## Setup

In a conda env with pytorch / cuda available, run:
```
pip install -r requirements.txt
```
Then in this repository:
```
pip install -e .
```

## Download

Once your request is approved, you will receive links to download the tokenizer and model files.
Edit the `download.sh` script with the signed url provided in the email to download the model weights and tokenizer.

## Inference

The provided `example.py` can be run on a single or multi-gpu node with `torchrun` and will output completions for two pre-defined prompts. 

In this repository, 4-GPU inference setting is considered. Using `TARGET_FOLDER` as defined in `download.sh`:
```
torchrun --nproc_per_node 4 example.py --ckpt_dir $TARGET_FOLDER --tokenizer_path $TARGET_FOLDER/tokenizer.model
```

## Reference

LLaMA: Open and Efficient Foundation Language Models -- https://arxiv.org/abs/2302.13971

```
@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
```

## Model Card
See [MODEL_CARD.md](MODEL_CARD.md)

## License
See the [LICENSE](LICENSE) file.
